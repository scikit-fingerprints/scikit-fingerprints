{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473e4b0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Dataset splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96a2ea",
   "metadata": {},
   "source": [
    "Splitting dataset into training and testing sets can be done in a few different ways. We can also split training set into train and validation part if we have enough data, instead of using cross-validation.\n",
    "\n",
    "scikit-fingerprints implements many split types specific for chemistry, which take into consideration molecular data itself. This can result in much more realistic split than simple random splitting.\n",
    "\n",
    "They are generally divided into two settings:\n",
    "1. **Internal / interpolative** testing, where we expect future data to be similar to training data in terms of distribution. For example, molecules would be similar structurally, in terms of physicochemical properties, bioactivity etc. to existing data. In other words, we want to test in-distribution performance of ML models.\n",
    "2. **External / extrapolative** testing, when we know that future data will substantially differ from training data. For example, we will work on novel structures, non-patented molecules, new chemical spaces etc. Therefore, we need to test **out-of-distribution (OOD)** generalization ability of ML models.\n",
    "\n",
    "Splitting methods for internal testing are random and MaxMin (maximum diversity) splitting, whereas scaffold and Butina splits are designed for extrapolative testing. Which split you want to use depends on the use case, what assumptions you make, and what kind of generalization you want to check.\n",
    "\n",
    "Let's go over those types of splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50656d53-e7e4-46d1-a5c9-56887de446f0",
   "metadata": {},
   "source": [
    "### Random split\n",
    "\n",
    "A typical splitting method, implemented in [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function in scikit-learn. It doesn't use data structure, but instead just randomly assigns it to train and test sets.\n",
    "\n",
    "It can use stratification, ensuring the same proportion of classes before splitting and in resulting splits, which is useful for imbalanced classification. Since it relies on randomness, we can do this multiple times with different `random_state` values set, and calculate e.g. standard deviation.\n",
    "\n",
    "However, it can overestimate performance if we expect novel data in the future (see e.g. [MoleculeNet paper](https://doi.org/10.1039/C7SC02664A), [here](https://arxiv.org/abs/1905.12265), [here](https://pubs.acs.org/doi/10.1021/ci400084k) and [here](https://doi.org/10.1021/ci200615h)). It's also susceptible to \"clumping\", i.e. natural clustering of molecules in chemical space (see e.g. [MUV dataset paper](https://doi.org/10.1021/ci8002649)), where training and testing molecules are very similar, which inflates the quality metric values. This is because random picking will more often sample from dense regions in chemical space, relatively to number of molecules. Therefore, performance estimation underestimates the importance of more sparsely sampled areas, which still may be very interesting, e.g. due to patentability.\n",
    "\n",
    "**Pros:**\n",
    "- simple\n",
    "- stratification\n",
    "- can use many random initializations\n",
    "\n",
    "**Cons:**\n",
    "- often overestimates performance\n",
    "- very similar molecules in train and test sets\n",
    "- susceptible to clumping\n",
    "- uneven sampling of chemical space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb06ca57-32ee-4c21-86dd-c69f65188acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:58:31.926155Z",
     "iopub.status.busy": "2025-01-31T12:58:31.925808Z",
     "iopub.status.idle": "2025-01-31T12:58:32.676623Z",
     "shell.execute_reply": "2025-01-31T12:58:32.676208Z",
     "shell.execute_reply.started": "2025-01-31T12:58:31.926129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC random split: 89.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skfp.datasets.moleculenet import load_bace\n",
    "from skfp.fingerprints import ECFPFingerprint\n",
    "\n",
    "smiles_list, y = load_bace()\n",
    "smiles_train, smiles_test, y_train, y_test = train_test_split(\n",
    "    smiles_list, y, random_state=0\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ECFPFingerprint(),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=0),\n",
    ")\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC random split: {auroc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4b0c0-b404-4667-a42b-53ccbb516610",
   "metadata": {},
   "source": [
    "### MinMax split\n",
    "\n",
    "Maximum diversity picking is a task of picking $k$ most diverse molecules from a set of $n$ molecules. This is typically practically implemented as selecting $k$ molecules with maximal sum of pairwise [Tanimoto distances](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-015-0069-3) between them, using ECFP4 as fingerprint representation. This way, we can select the maximally diverse test set. scikit-fingerprints uses [MaxMin algorithm](https://rdkit.blogspot.com/2017/11/revisting-maxminpicker.html) from RDKit, which is an efficient approximation (exact solution [is NP-hard](https://grafo.etsii.urjc.es/optsicom/mdp.html)). Publications using this split include e.g. [ApisTox](https://doi.org/10.1038/s41597-024-04232-w), [this paper](https://doi.org/10.1002/qsar.200290002) and [this paper](https://doi.org/10.1186/s12859-024-05643-7).\n",
    "\n",
    "Picking maximally diverse molecules results in relatively uniform coverage of whole chemical space in the dataset. This alleviates problems like clumping and undersampling sparse areas. It validates the internal generalization performance across the whole spectrum of molecular structures available. Since this is an approximation, relying on random starting molecule, it can also use `random_state` to obtain different split variants.\n",
    "\n",
    "The performance estimation can still be influenced by very similar molecules in train and test sets. It also does not consider class distribution, and for very imbalanced datasets can pick almost only the majority class.\n",
    "\n",
    "**Pros:**\n",
    "- measures performance uniformly in the entire chemical space\n",
    "- resistant to clumping and sparse areas\n",
    "- more robust internal measure than random split\n",
    "- can use many random initializations\n",
    "\n",
    "**Cons:**\n",
    "- can have very similar molecules in train and test sets\n",
    "- may not work well for heavily imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e676863e-6504-4706-98c1-fce2fcc23287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:18:31.580301Z",
     "iopub.status.busy": "2025-01-31T12:18:31.580124Z",
     "iopub.status.idle": "2025-01-31T12:18:32.601587Z",
     "shell.execute_reply": "2025-01-31T12:18:32.601142Z",
     "shell.execute_reply.started": "2025-01-31T12:18:31.580284Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC MaxMin split: 88.42%\n"
     ]
    }
   ],
   "source": [
    "from skfp.model_selection import maxmin_train_test_split\n",
    "\n",
    "smiles_train, smiles_test, y_train, y_test = maxmin_train_test_split(smiles_list, y)\n",
    "\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC MaxMin split: {auroc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1bd95-3701-4c1d-a0fc-1193f21f4ea2",
   "metadata": {},
   "source": [
    "### Scaffold split\n",
    "\n",
    "Scaffold split divides molecules based on their [Bemis-Murcko scaffold](https://docs.chemaxon.com/display/docs/jklustor_bemis-murcko-clustering.md). Scaffold is a \"backbone\" of the molecule, built from its connected ring systems and using only carbons. The idea is that molecules with the same scaffold have the same general structure and shape, differing only by substituents on the \"edges\" of the molecule. In scaffold split, we first group molecules by their scaffold, and the test set is made by combining the smallest scaffold groups. This way, we test on the most atypical, rare scaffolds, requiring out-of-distribution generalization to structurally novel molecules.\n",
    "\n",
    "Using it for train-test splitting has been proposed in [MoleculeNet paper](https://doi.org/10.1039/C7SC02664A), and has been widely adopted, being arguably the most popular split in molecular ML nowadays. This split is very fast to compute, in constrast to many external testing splits. It typically results in more realistic performance estimation than random split, particularly for tasks requiring novel molecule design.\n",
    "\n",
    "However, it is [susceptible to small changes](https://practicalcheminformatics.blogspot.com/2024/11/some-thoughts-on-splitting-chemical.html) in the scaffold atoms, where almost identical molecules can differ by a single atom and get different scaffolds. This is a consequence by rigidly defining similarity as \"identical scaffold or not\", instead of answering a general question how similar structurally are two molecules. For further discussion, see e.g. [RDKit blog](https://greglandrum.github.io/rdkit-blog/posts/2024-05-31-scaffold-splits-and-murcko-scaffolds1.html), [this blog post](https://www.blopig.com/blog/2024/12/a-tougher-molecular-data-split-spectral-split/), and [this paper](https://arxiv.org/abs/2406.00873). In the canonical version, it also does not work for molecules with disconnected components, e.g. salts. RDKit and scikit-fingerprints allow it, using predefined ordering, but proper scaffold is not well defined in those cases. It is fully deterministic, and there is only a single train-test split possible.\n",
    "\n",
    "There are also some variants on the scaffold definition, which can be sometimes useful, but can be challenging for reproducible benchmarks. While original Bemis-Murcko scaffold is very \"generic\" and uses carbon-only connected ring systems (CSK, Cyclic SKeleton), RDKit scaffolds include some substituent atoms. See [this RDKit discussion](https://github.com/rdkit/rdkit/discussions/6844) for details. scikit-fingerprints uses RDKit version by default, but you can use CDK with `use_csk` parameter.\n",
    "\n",
    "**Pros:**\n",
    "- fast\n",
    "- popular\n",
    "- tests for structurally novel molecules\n",
    "- typically more challenging than random split\n",
    "- deterministic\n",
    "\n",
    "**Cons:**\n",
    "- susceptible to small changes in scaffold\n",
    "- arguably not very realistic and challenging\n",
    "- differing scaffold definitions\n",
    "- not well-defined for disconnected molecules\n",
    "- only a single train-test split possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0274b8ac-8ff5-45be-af0c-fa1f55488373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:52:05.420563Z",
     "iopub.status.busy": "2025-01-31T12:52:05.420205Z",
     "iopub.status.idle": "2025-01-31T12:52:06.499353Z",
     "shell.execute_reply": "2025-01-31T12:52:06.498906Z",
     "shell.execute_reply.started": "2025-01-31T12:52:05.420536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC scaffold split: 78.25%\n"
     ]
    }
   ],
   "source": [
    "from skfp.model_selection import scaffold_train_test_split\n",
    "\n",
    "smiles_train, smiles_test, y_train, y_test = scaffold_train_test_split(smiles_list, y)\n",
    "\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC scaffold split: {auroc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26401fbe-3bbc-4dae-b018-b0c4ff813fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:52:16.819219Z",
     "iopub.status.busy": "2025-01-31T12:52:16.818613Z",
     "iopub.status.idle": "2025-01-31T12:52:18.371434Z",
     "shell.execute_reply": "2025-01-31T12:52:18.370996Z",
     "shell.execute_reply.started": "2025-01-31T12:52:16.819188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC CSK scaffold split: 84.55%\n"
     ]
    }
   ],
   "source": [
    "from skfp.model_selection import scaffold_train_test_split\n",
    "\n",
    "smiles_train, smiles_test, y_train, y_test = scaffold_train_test_split(\n",
    "    smiles_list, y, use_csk=True\n",
    ")\n",
    "\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC CSK scaffold split: {auroc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feeffdf-5ad2-4c91-a164-a70687f5b4a9",
   "metadata": {},
   "source": [
    "### Randomized scaffold split\n",
    "\n",
    "Scaffold split is fully deterministic, putting smallest scaffold groups in the test set. However, we can also divide them randomly between train and test results, resulting in randomized scaffold split. It is also known as [\"balanced\" scaffold split](https://proceedings.neurips.cc/paper_files/paper/2022/hash/4ec360efb3f52643ac43fda570ec0118-Abstract-Conference.html).\n",
    "\n",
    "It can be run multiple times with different `random_state`, allowing calculation of e.g. standard deviation. We can also be interested in more popular scaffold generalization, rather than just the rarest ones.\n",
    "\n",
    "However, the performance estimation is much more optimistic in this variant. This is because \"simpler\", larger groups of scaffolds can easily dominate the test set. Furthermore, some authors unfortunately mix up this variant and regular, more challenging scaffold split, e.g. in [GROVER paper](https://proceedings.neurips.cc/paper/2020/hash/94aef38441efa3380a3bed3faf1f9d5d-Abstract.html), without any proper distinction. See Appendix G in [MOLTOP paper](https://arxiv.org/abs/2407.12136) for discussion. This can [inflate the results](https://proceedings.neurips.cc/paper_files/paper/2022/hash/4ec360efb3f52643ac43fda570ec0118-Abstract-Conference.html) quite a lot.\n",
    "\n",
    "**Pros:**\n",
    "- can use many random initializations\n",
    "\n",
    "**Cons:**\n",
    "- performance estimation can be too optimistic\n",
    "- often confused with scaffold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62cfd8de-8eea-45ad-a679-035fe4ffbde5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T12:58:42.968018Z",
     "iopub.status.busy": "2025-01-31T12:58:42.967505Z",
     "iopub.status.idle": "2025-01-31T12:58:44.126564Z",
     "shell.execute_reply": "2025-01-31T12:58:44.126086Z",
     "shell.execute_reply.started": "2025-01-31T12:58:42.967992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC randomized scaffold split: 87.49%\n"
     ]
    }
   ],
   "source": [
    "from skfp.model_selection import randomized_scaffold_train_test_split\n",
    "\n",
    "smiles_train, smiles_test, y_train, y_test = randomized_scaffold_train_test_split(\n",
    "    smiles_list, y, random_state=0\n",
    ")\n",
    "\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC randomized scaffold split: {auroc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6136d-65b7-467c-a42e-c3b0a469eb79",
   "metadata": {},
   "source": [
    "### Butina split\n",
    "\n",
    "Butina split applies [Taylor-Butina clustering](https://projects.volkamerlab.org/teachopencadd/talktorials/T005_compound_clustering.html#Detailed-explanation-of-Butina-clustering) to cluster together similar molecules, and then assigns the smallest clusters to the test set. Typically ECFP4 fingerprints are used as features. As a density-based clustering, it can detect clusters of varied shapes and sizes. It typically results in a large numer of small clusters, since it uses Tanimoto similarity threshold to limit maximal allowed dissimilarity in a cluster.\n",
    "\n",
    "It can be seen as a more flexible alternative to scaffold split, using more complex structural similarity measure (ECFP + Tanimoto, instead of identical Bemis-Murcko scaffolds). As such, it is often more realistic and challenging.\n",
    "\n",
    "However, the computational cost is quite high, as it requires computing full $O(n^2)$ similarity matrix in the worst case. scikit-fingerprints uses efficient [Leader Clustering](https://www.nextmovesoftware.com/talks/Sayle_2DSimilarityDiversityAndClusteringInRdkit_RDKITUGM_201909.pdf) implementation from RDKit, but scaling is still unfavorable for large datasets.\n",
    "\n",
    "A tradeoff between accuracy is cost is the approximate solution, using [NNDescent algorithm](https://doi.org/10.1145/1963405.1963487) for computing approximate nearest neighbors. It requires installing [PyNNDescent library](https://pynndescent.readthedocs.io/en/latest/).\n",
    "\n",
    "**Pros:**\n",
    "- flexible\n",
    "- tests for structurally novel molecules\n",
    "- challenging out-of-distribution split\n",
    "- deterministic\n",
    "- approximate version available\n",
    "\n",
    "**Cons:**\n",
    "- computationally expensive\n",
    "- only a single train-test split possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c741c5a7-33b1-43b1-89eb-4a1520fb1fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T13:19:19.696423Z",
     "iopub.status.busy": "2025-01-31T13:19:19.696165Z",
     "iopub.status.idle": "2025-01-31T13:19:21.061956Z",
     "shell.execute_reply": "2025-01-31T13:19:21.061371Z",
     "shell.execute_reply.started": "2025-01-31T13:19:19.696404Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet pynndescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf3161b-00b2-49bb-aae9-30b6a94b0f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T13:18:14.203135Z",
     "iopub.status.busy": "2025-01-31T13:18:14.202614Z",
     "iopub.status.idle": "2025-01-31T13:18:16.128607Z",
     "shell.execute_reply": "2025-01-31T13:18:16.128195Z",
     "shell.execute_reply.started": "2025-01-31T13:18:14.203107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC Butina split: 80.25%\n"
     ]
    }
   ],
   "source": [
    "from skfp.model_selection import butina_train_test_split\n",
    "\n",
    "smiles_train, smiles_test, y_train, y_test = butina_train_test_split(smiles_list, y)\n",
    "\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC Butina split: {auroc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f98d12-76a2-4f57-b49b-26ef1397686f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T13:19:36.883566Z",
     "iopub.status.busy": "2025-01-31T13:19:36.883202Z",
     "iopub.status.idle": "2025-01-31T13:19:38.912784Z",
     "shell.execute_reply": "2025-01-31T13:19:38.912250Z",
     "shell.execute_reply.started": "2025-01-31T13:19:36.883535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC approximate Butina split: 80.26%\n"
     ]
    }
   ],
   "source": [
    "from skfp.model_selection import butina_train_test_split\n",
    "\n",
    "smiles_train, smiles_test, y_train, y_test = butina_train_test_split(\n",
    "    smiles_list, y, approximate=True\n",
    ")\n",
    "\n",
    "pipeline.fit(smiles_train, y_train)\n",
    "y_pred = pipeline.predict_proba(smiles_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUROC approximate Butina split: {auroc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
